import re
from concurrent.futures import ThreadPoolExecutor
from pydantic import BaseModel, Field

model_dict = {
    'vicuna-13b': {
        'ip': 'http://localhost:5031/',
        'limit': 10
    },
    'stablelm-tuned-alpha-7b': {
        'ip': 'http://localhost:5032/',
        'limit': 10
    },
    'dolly-v2-12b': {
        'ip': 'http://localhost:5033/',
        'limit': 10
    },
    'llama-2-70b-chat': {
        'ip': 'http://localhost:5034/',
        'limit': 10
    },
    'codellama-13b-instruct': {
        'ip': 'http://localhost:5035/',
        'limit': 10
    },
    # following models only used for testing or debug
    'fake-model': {
        'ip': 'http://localhost:6000/',
        'limit': 10
    },
    'flan-t5-small': {
        'ip': 'http://localhost:6001/',
        'limit': 10
    },
}

default_args_dict = {
    'return_only_text': True
}

GLOBAL_THREAD_POOL = ThreadPoolExecutor(max_workers=20)

# generation params
gen_params = {
    "early_stopping": False,
    "no_repeat_ngram_size": 0,
    "do_sample": False,
    "min_new_tokens": 0,
    "num_beams": 1,
    "top_k": 50,
    "diversity_penalty": 0.,
    "repetition_penalty": 1.,
    "max_new_tokens": 1024,
    "temperature": 1.,
    "top_p": 1.,
}


class ErrorResponse(BaseModel):
    code: int
    message: str


class PredictOutput(BaseModel):
    model: str = Field(..., description="The Model ID that was used")
    usage: dict = Field(..., description="Usage details including number of tokens used during processing, same format as OpenAI")
    id: str = Field(..., description="Response ID")
    object: str = Field(..., description="Type of response")
    created: int = Field(..., description="Time of response creation")
    choices: dict = Field(..., description="Text generated by the model")


class PredictInput(BaseModel):
    model: str = Field(..., description="The Model ID to use")
    messages: dict = Field(..., description="List of previous chat messages, same format as OpenAI")
    stream: bool = Field(False, description="Decide if results should be streamed or not")
    return_only_text: bool = Field(False, description="Whether or not to only return text, used only if Stream is True")
    stop: list = Field([], description="List of stop words to prematurely end the generation")
    temperature: float = Field(1., description="Model temperature")
